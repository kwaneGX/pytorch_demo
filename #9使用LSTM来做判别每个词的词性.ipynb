{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']), (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "print training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Everybody': 5, 'ate': 2, 'apple': 4, 'that': 7, 'read': 6, 'dog': 1, 'book': 8, 'the': 3, 'The': 0} {'DET': 0, 'NN': 1, 'V': 2}\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "for context, tag in training_data:\n",
    "    for word in context:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "    for label in tag:\n",
    "        if label not in tag_to_idx:\n",
    "            tag_to_idx[label] = len(tag_to_idx)\n",
    "print word_to_idx,tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "character_to_idx = {}\n",
    "for i in range(len(alphabet)):\n",
    "    character_to_idx[alphabet[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, n_char, char_dim, char_hidden):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.char_embedding = nn.Embedding(n_char, char_dim)\n",
    "        self.char_lstm = nn.LSTM(char_dim, char_hidden, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.char_embedding(x)\n",
    "        _, h = self.char_lstm(x)\n",
    "        return h[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, n_word, n_char, char_dim, n_dim, char_hidden,\n",
    "                 n_hidden, n_tag):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(n_word, n_dim)\n",
    "        self.char_lstm = CharLSTM(n_char, char_dim, char_hidden)\n",
    "        self.lstm = nn.LSTM(n_dim+char_hidden, n_hidden, batch_first=True)\n",
    "        self.linear1 = nn.Linear(n_hidden, n_tag)\n",
    "\n",
    "    def forward(self, x, word_data):\n",
    "        word = [i for i in word_data]\n",
    "        char = torch.FloatTensor()\n",
    "        for each in word:\n",
    "            word_list = []\n",
    "            for letter in each:\n",
    "                word_list.append(character_to_idx[letter.lower()])\n",
    "            word_list = torch.LongTensor(word_list)\n",
    "            word_list = word_list.unsqueeze(0)\n",
    "            tempchar = self.char_lstm(Variable(word_list).cuda())\n",
    "            tempchar = tempchar.squeeze(0)\n",
    "            char = torch.cat((char, tempchar.cpu().data), 0)\n",
    "        char = char.squeeze(1)\n",
    "        char = Variable(char).cuda()\n",
    "        x = self.word_embedding(x)\n",
    "        x = torch.cat((x, char), 1)\n",
    "        x = x.unsqueeze(0)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.linear1(x)\n",
    "        y = F.log_softmax(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
